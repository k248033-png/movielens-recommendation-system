# -*- coding: utf-8 -*-
"""ALS_Recommendation_System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TE20AGRZpT5RPHg9BjEQNj4s4j1MXilx
"""

# Install required packages
!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz
!tar xf spark-3.3.0-bin-hadoop3.tgz
!pip install -q findspark

# Set environment variables
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.3.0-bin-hadoop3"

import findspark
findspark.init()

# Initialize Spark
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.ml.recommendation import ALS
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
import pandas as pd
import numpy as np

spark = SparkSession.builder \
    .appName("MovieRecommendationSystem") \
    .config("spark.executor.memory", "2g") \
    .config("spark.driver.memory", "2g") \
    .getOrCreate()

# Load the dataset
ratings_df = spark.read.csv(
    "/content/ratings.csv",
    header=True,
    inferSchema=True
)

movies_df = spark.read.csv(
    "/content/movies.csv",
    header=True,
    inferSchema=True
)

print("Missing values in ratings:")
ratings_df.select([count(when(col(c).isNull(), c)).alias(c) for c in ratings_df.columns]).show()

print("\nMissing values in movies:")
movies_df.select([count(when(col(c).isNull(), c)).alias(c) for c in movies_df.columns]).show()

# Check data types
print("\nData types before conversion:")
ratings_df.dtypes

# Convert timestamp
from pyspark.sql.functions import from_unixtime

ratings_df = ratings_df.withColumn(
    "timestamp_formatted",
    from_unixtime(col("timestamp")).cast("timestamp")
)

# Remove duplicate ratings
print(f"Ratings before removing duplicates: {ratings_df.count():,}")
ratings_df = ratings_df.dropDuplicates(["userId", "movieId"])
print(f"Ratings after removing duplicates: {ratings_df.count():,}")

# Filter out movies
rating_counts = ratings_df.groupBy("movieId").count()
movies_with_enough_ratings = rating_counts.filter(col("count") >= 5)

print(f"\nMovies with at least 5 ratings: {movies_with_enough_ratings.count():,}")

# Join to filter ratings
ratings_df = ratings_df.join(
    movies_with_enough_ratings.select("movieId"),
    "movieId",
    "inner"
)

# Statistics
print("Rating Statistics:")
ratings_df.select("rating").describe().show()

# Distribution of ratings
print("\nRating Distribution:")
ratings_df.groupBy("rating").count().orderBy("rating").show(20)

# Visualize rating distribution
import matplotlib.pyplot as plt
import seaborn as sns

rating_dist = ratings_df.groupBy("rating").count().orderBy("rating").toPandas()

plt.figure(figsize=(12, 6))
plt.bar(rating_dist['rating'], rating_dist['count'])
plt.title('Distribution of Movie Ratings')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.xticks([0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0])
plt.grid(True, alpha=0.3)
plt.show()

# Top 10 most rated movies
print("\nTop 10 Most Rated Movies:")
top_rated = ratings_df.join(movies_df, "movieId") \
    .groupBy("movieId", "title") \
    .agg(
        count("rating").alias("num_ratings"),
        avg("rating").alias("avg_rating")
    ) \
    .orderBy(col("num_ratings").desc()) \
    .limit(10)

top_rated.show(truncate=False)

# Top 10 highest rated movies
print("\nTop 10 Highest Rated Movies (min 50 ratings):")
highest_rated = ratings_df.join(movies_df, "movieId") \
    .groupBy("movieId", "title") \
    .agg(
        count("rating").alias("num_ratings"),
        avg("rating").alias("avg_rating")
    ) \
    .filter(col("num_ratings") >= 50) \
    .orderBy(col("avg_rating").desc()) \
    .limit(10)

highest_rated.show(truncate=False)

print("\nUser Rating Statistics:")
user_stats = ratings_df.groupBy("userId") \
    .agg(
        count("rating").alias("num_ratings"),
        avg("rating").alias("avg_user_rating"),
        min("rating").alias("min_rating"),
        max("rating").alias("max_rating")
    ) \
    .orderBy(col("num_ratings").desc())

user_stats.show(10)

print(f"\nAverage ratings per user: {user_stats.select(avg('num_ratings')).first()[0]:.2f}")
print(f"Most active user rated: {user_stats.select(max('num_ratings')).first()[0]} movies")

# Split data for training and testing
print("Splitting data into training (80%) and test (20%) sets...")
(training, test) = ratings_df.randomSplit([0.8, 0.2], seed=42)

print(f"Training set: {training.count():,} ratings")
print(f"Test set: {test.count():,} ratings")

# ALS model
print("\nBuilding ALS collaborative filtering model...")
als = ALS(
    maxIter=10,
    rank=10,
    regParam=0.01,
    userCol="userId",
    itemCol="movieId",
    ratingCol="rating",
    coldStartStrategy="drop",  # Drop NaN Values
    nonnegative=True,
    seed=42
)

# Train the model
model = als.fit(training)

# Make predictions
predictions = model.transform(test)

# Filter out NaN predictions
predictions = predictions.filter(col("prediction").isNotNull())

# Evaluate the model
evaluator = RegressionEvaluator(
    metricName="rmse",
    labelCol="rating",
    predictionCol="prediction"
)

rmse = evaluator.evaluate(predictions)
print(f"\nRoot Mean Square Error (RMSE): {rmse:.4f}")

print("\nSample predictions (first 20):")
predictions.select("userId", "movieId", "rating", "prediction",
                   (col("rating") - col("prediction")).alias("error")) \
    .show(20)

# Top 10 recommendations for all users
print("Generating recommendations for all users...")
user_recs = model.recommendForAllUsers(10)

# Recommendations for user 1
print("\nTop 10 recommendations for User 1:")
user1_recs = user_recs.filter(col("userId") == 1)
user1_recs.show(truncate=False)

def get_user_recommendations(user_id, num_recs=10):
    """Get personalized recommendations for a specific user"""

    # User's recommendations
    user_recs = model.recommendForAllUsers(num_recs) \
        .filter(col("userId") == user_id)

    # Extract recommendations
    from pyspark.sql.functions import explode

    recommendations = user_recs \
        .select(explode("recommendations").alias("rec")) \
        .select("rec.movieId", "rec.rating")

    # Join with movie details
    recommendations_with_details = recommendations \
        .join(movies_df, "movieId") \
        .select("movieId", "title", "genres", col("rating").alias("predicted_rating")) \
        .orderBy(col("predicted_rating").desc())

    return recommendations_with_details

# Recommendations for user 1
user1_recommendations = get_user_recommendations(1, 10)
print(f"\nPersonalized recommendations for User 1:")
user1_recommendations.show(truncate=False)

# For comparison
print("\nMovies rated by User 1:")
user_rated = ratings_df.filter(col("userId") == 1) \
    .join(movies_df, "movieId") \
    .select("movieId", "title", "genres", "rating") \
    .orderBy(col("rating").desc())

user_rated.show(10, truncate=False)

# Evaluation metrics
from pyspark.ml.evaluation import RegressionEvaluator

evaluators = {
    "RMSE": RegressionEvaluator(metricName="rmse", labelCol="rating", predictionCol="prediction"),
    "MAE": RegressionEvaluator(metricName="mae", labelCol="rating", predictionCol="prediction"),
    "R2": RegressionEvaluator(metricName="r2", labelCol="rating", predictionCol="prediction")
}

print("Model Evaluation Metrics:")
for metric_name, evaluator in evaluators.items():
    score = evaluator.evaluate(predictions)
    print(f"{metric_name}: {score:.4f}")

model_path = "/content/movie_recommendation_als_model"
model.save(model_path)
print(f"Model saved to: {model_path}")


user_recs_path = "/content/user_recommendations"
user_recs.write.parquet(user_recs_path)
print(f"User recommendations saved to: {user_recs_path}")

from pyspark.ml.recommendation import ALSModel
loaded_model = ALSModel.load(model_path)
loaded_user_recs = spark.read.parquet(user_recs_path)

print("Model and recommendations can be loaded for future use")

# Recommendation function
def recommend_movies_for_user(user_id, n_recommendations=10, include_rated=False):
    """
    Complete recommendation function for a user
    """

    # Get user's watched movies
    watched_movies = ratings_df.filter(col("userId") == user_id) \
        .join(movies_df, "movieId") \
        .select("movieId", "title", "genres", "rating")

    # Get recommendations
    recommendations = get_user_recommendations(user_id, n_recommendations)

    # Filter out already watched movies
    if not include_rated:
        watched_ids = [row.movieId for row in watched_movies.select("movieId").collect()]
        recommendations = recommendations.filter(~col("movieId").isin(watched_ids))

    return watched_movies, recommendations

print("Complete recommendation pipeline for User 1:")
watched, new_recommendations = recommend_movies_for_user(1, 15, include_rated=False)

print("\nMovies User 1 has watched:")
watched.orderBy(col("rating").desc()).show(10, truncate=False)

print(f"\nNew recommendations for User 1 ({new_recommendations.count()} movies):")
new_recommendations.show(truncate=False)

# Dataset summary
print("=== DATASET SUMMARY ===")
print(f"Total Users: {ratings_df.select('userId').distinct().count():,}")
print(f"Total Movies: {movies_df.count():,}")
print(f"Total Ratings: {ratings_df.count():,}")
print(f"Average ratings per user: {ratings_df.count() / ratings_df.select('userId').distinct().count():.2f}")
print(f"Average ratings per movie: {ratings_df.count() / ratings_df.select('movieId').distinct().count():.2f}")
print(f"Rating range: {ratings_df.select(min('rating')).first()[0]} to {ratings_df.select(max('rating')).first()[0]}")

# Model Summary
print("\n=== MODEL PERFORMANCE ===")
print(f"RMSE: {rmse:.4f}")
print(f"Model trained on: {training.count():,} ratings")
print(f"Model tested on: {test.count():,} ratings")

spark.stop()
print("\nSpark session stopped. Process completed!")